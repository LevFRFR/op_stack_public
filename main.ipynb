{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# import environment variables\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load text\n",
    "\n",
    "Let's use simple pdf document loader `PDFMinerLoader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFMinerLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_pdf = r'../raw data/2022-climate-report-en.pdf'\n",
    "annual_pdf = r'../raw data/2022-annual-report-en.pdf'\n",
    "sustainability_pdf = r'../raw data/2022-report-on-sustainability-en.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "\n",
    "for report in [climate_pdf,annual_pdf,sustainability_pdf]:\n",
    "    loader = PDFMinerLoader(report)\n",
    "    data_list.append( loader.load() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in hard facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "txts = []\n",
    "\n",
    "for txt_name in ['environment', 'governance', 'social']:\n",
    "    with open(r'D:\\python_projects\\suncor_OP\\hard_facts\\{}.txt'.format(txt_name)) as f:\n",
    "        txts.append( f.read() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split text\n",
    "\n",
    "Splitting text by `TiktokenTextSplitter`, which is based on OpenAI's ADA tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = 'cl100k_base'\n",
    "CHUNK_SIZE = 800\n",
    "\n",
    "text_splitter = TokenTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=100, encoding_name=TOKENIZER)\n",
    "# texts = text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "small_text_splitter = CharacterTextSplitter.from_tiktoken_encoder(chunk_size=100, chunk_overlap=20, encoding_name=TOKENIZER)\n",
    "hard_fact_texts = []\n",
    "for txt in txts:\n",
    "    hard_fact_texts.append( small_text_splitter.split_text(txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_fact_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch text splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_docs_list = []\n",
    "\n",
    "for data in data_list:\n",
    "    additional_docs_list.append( text_splitter.split_documents(data) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in additional_docs_list:\n",
    "    print(f'Total Number of Chunks: {len(doc)}, at {CHUNK_SIZE} max token input.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Splitter at page number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.text_splitter import TextSplitter\n",
    "# import re\n",
    "\n",
    "# pdf_full_text = data[0].page_content\n",
    "\n",
    "# # Define regular expression to match the template string and the following number\n",
    "# regex = re.compile(r\"Suncor Energy Inc\\.   \\|   Climate Report 2022   \\|   (\\d+)\")\n",
    "\n",
    "# # Split the input string at the regex pattern\n",
    "# split_str = regex.split(pdf_full_text)\n",
    "\n",
    "# page_nums = [1]\n",
    "# page_cont = []\n",
    "# for idx, str in enumerate(split_str):\n",
    "#     if idx % 2 == 0:\n",
    "#         page_cont.append(str)\n",
    "#     else:\n",
    "#         page_nums.append(int(str))\n",
    "\n",
    "# dict(zip(page_nums, page_cont))\n",
    "\n",
    "# # text_splitter = TextSplitter(chunk_size=4000, chunk_overlap=0)\n",
    "\n",
    "# # Custom text splitter at the page\n",
    "# # docs = text_splitter.create_documents(texts=page_cont, metadatas=[{'page':page_nums}])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create embeddings\n",
    "\n",
    "Using OpenAI's embedding's model `text-embedding-ada-002`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import openai\n",
    "\n",
    "# MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "# # get API key from top-right dropdown on OpenAI website\n",
    "\n",
    "# # input is a list of strings, exactly like texts\n",
    "# res = openai.Embedding.create(\n",
    "#     input=texts, \n",
    "#     engine=MODEL\n",
    "# )\n",
    "\n",
    "# # extract embeddings to a list\n",
    "# embeds = [record['embedding'] for record in res['data']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Pinecone vectorstore\n",
    "\n",
    "Initializing vectorstore is same as connecting to a Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENV')  # find next to API key in console\n",
    ")\n",
    "\n",
    "# connect to index\n",
    "index = pinecone.Index(os.getenv('PINECONE_INDEX'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pinecone.delete_index(os.getenv('PINECONE_INDEX'))\n",
    "\n",
    "# # delete all from index\n",
    "# index.delete(delete_all=True, namespace='suncor')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate Pinecone vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "index_name = os.getenv('PINECONE_INDEX')\n",
    "\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv('OPENAI_API_KEY'), model=MODEL) # add api key and model if using langchain's wrapper of openai\n",
    "\n",
    "# pine_vector_store = Pinecone.from_documents(docs, embeddings, index_name=index_name, namespace='suncor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_fact_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for txt_list in hard_fact_texts:\n",
    "    Pinecone.from_texts(\n",
    "        txt_list, \n",
    "        embedding=embeddings, \n",
    "        index_name=index_name, \n",
    "        namespace='suncor'\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populating additional docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in additional_docs_list:\n",
    "    Pinecone.from_documents(doc, embeddings, index_name=index_name, namespace='suncor')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# import environment variables\n",
    "load_dotenv('../.env')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to existing Pinecone index\n",
    "\n",
    "Using `from_existing_index`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_projects\\.conda\\envs\\op\\Lib\\site-packages\\pinecone\\index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import os\n",
    "import pinecone\n",
    "\n",
    "\n",
    "MODEL = \"text-embedding-ada-002\"\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "pinecone.init(\n",
    "    api_key=os.getenv('PINECONE_API_KEY'),\n",
    "    environment=os.getenv('PINECONE_ENV')  # find next to API key in console\n",
    ")\n",
    "\n",
    "pine_vector_store = Pinecone.from_existing_index(os.getenv('PINECONE_INDEX'), OpenAIEmbeddings(model=MODEL))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create chain using pinecone's vectorstore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question answering with Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "QUERY = \"What color is a firetruck in Canada?\"\n",
    "\n",
    "similar_docs_with_score = pine_vector_store.similarity_search_with_score(QUERY, namespace='suncor')\n",
    "\n",
    "similar_docs_only = tuple(doc for doc, sim_score in similar_docs_with_score)\n",
    "similarity_scores = tuple(sim_score for doc, sim_score in similar_docs_with_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python_projects\\.conda\\envs\\op\\Lib\\site-packages\\langchain\\llms\\openai.py:158: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "d:\\python_projects\\.conda\\envs\\op\\Lib\\site-packages\\langchain\\llms\\openai.py:667: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "PROMPT_TEMPLATE = '''You are a helpful AI assistant. Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say you don't know. DO NOT try to make up an answer.\n",
    "If the question is not related to the context, politely respond that you are tuned to only answer questions that are related to the context.\n",
    "\n",
    "{summaries}\n",
    "\n",
    "Question: {question}\n",
    "Helpful answer:'''\n",
    "\n",
    "PROMPT = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=[\"summaries\", \"question\"])\n",
    "\n",
    "DOCUMENT_PROMPT = PromptTemplate(\n",
    "    template=\"Content: {page_content}\",\n",
    "    input_variables=[\"page_content\"],\n",
    ")\n",
    "\n",
    "chain = load_qa_with_sources_chain(\n",
    "    OpenAI(temperature=0, model_name=MODEL), \n",
    "    chain_type=\"stuff\", \n",
    "    prompt=PROMPT,\n",
    "    document_prompt=DOCUMENT_PROMPT\n",
    ")\n",
    "\n",
    "resp = chain({\"input_documents\": similar_docs_only, \"question\": QUERY})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I am tuned to only answer questions related to the provided context. I do not have information on the color of firetrucks in Canada.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp['output_text']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Question/Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pine_vector_store = Pinecone.from_existing_index(os.getenv('PINECONE_INDEX'), OpenAIEmbeddings(model=MODEL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pine_vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "MODEL = 'gpt-3.5-turbo'\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=OpenAI(model_name=MODEL, temperature=0), \n",
    "    chain_type=\"stuff\",\n",
    "    retriever=pine_vector_store.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is SAGD?\"\n",
    "qa.run(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "op",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
